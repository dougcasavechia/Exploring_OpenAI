{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geracao_resposta_simultaneo(mensagens, model='gpt-3.5-turbo-0125', max_tokens=1000, temperature=0, stream=True):\n",
    "    \"\"\"\n",
    "    Função para gerar texto usando a API do ChatGPT com base nas mensagens passadas.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - mensagens: lista de dicionários que contém as mensagens anteriores (conversa).\n",
    "    - model: o modelo de linguagem a ser usado (padrão: 'gpt-3.5-turbo-0125').\n",
    "    - max_tokens: o número máximo de tokens que a resposta pode ter (padrão: 1000).\n",
    "    - temperature: define a aleatoriedade da resposta (padrão: 0, mais determinístico).\n",
    "    - stream: se True, a resposta é transmitida em tempo real, recebendo pedaços do conteúdo conforme ele é gerado.\n",
    "    \"\"\"\n",
    "\n",
    "    print('ChatGPT', end=\"\")\n",
    "    resposta_completa = \"\"\n",
    "\n",
    "    # Fazendo uma solicitação à API do ChatGPT com base nas mensagens e parâmetros fornecidos.\n",
    "    resposta = client.chat.completions.create(\n",
    "        messages=mensagens,\n",
    "        model=model,  # Modelo de linguagem usado (exemplo: 'gpt-3.5-turbo-0125')\n",
    "        max_tokens=max_tokens,  # Limita o tamanho da resposta\n",
    "        temperature=temperature,  # Controla a criatividade da resposta (0 = mais previsível)\n",
    "        stream=stream\n",
    "    )\n",
    "\n",
    "    for stream_resposta in resposta:\n",
    "        texto = stream_resposta.choices[0].delta.content\n",
    "\n",
    "        if texto:\n",
    "            resposta_completa += texto\n",
    "            # time.sleep(0.1)\n",
    "            print(texto, end=\"\")\n",
    "\n",
    "    mensagens = mensagens.append({'role': 'assistant', 'content': resposta_completa})\n",
    "\n",
    "    return mensagens, resposta_completa\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    mensagens = []\n",
    "\n",
    "    while True:\n",
    "        input_usuario = input('User: ')\n",
    "        mensagens.append({'role': 'user', 'content': input_usuario})\n",
    "        geracao_resposta_simultaneo(mensagens)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
