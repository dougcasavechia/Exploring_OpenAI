{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv  # Para carregar variáveis de ambiente do arquivo .env\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando as variáveis de ambiente a partir do arquivo .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando as variáveis de ambiente a partir do arquivo .env\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Recuperando a chave da API OpenAI armazenada no arquivo .env\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurando a chave da API para autenticar solicitações à OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo o cliente para interagir com a API OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando uma mensagem de exemplo para a API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui, simulamos uma pergunta do usuário sobre uma maçã\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em 5 palavras?'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazendo uma solicitação para o modelo 'gpt-3.5-turbo-0125'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo uma solicitação para o modelo 'gpt-3.5-turbo-0125'\n",
    "# max_tokens: o máximo de tokens que a resposta pode ter\n",
    "# temperature: controla a criatividade (0 = mais determinístico, menos criativo)\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "    messages=mensagens,\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    max_tokens=1000,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exibindo a resposta gerada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruta redonda e saborosa.\n"
     ]
    }
   ],
   "source": [
    "print(resposta.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando a resposta do assistente à lista de mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função append insere a resposta do modelo como se fosse o \"assistant\" respondendo\n",
    "mensagens.append({'role': 'assistant', 'content': resposta.choices[0].message.content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando uma nova mensagem do usuário à lista de mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa mensagem simula o usuário perguntando \"E qual é a sua cor?\"\n",
    "# A mensagem é adicionada com o papel ('role') de 'user', mantendo o fluxo da conversa.\n",
    "mensagens.append({'role': 'user', 'content': 'E qual é a sua cor?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exibindo a respostas geradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'O que é uma maça em 5 palavras?'},\n",
       " {'role': 'assistant', 'content': 'Fruta redonda e saborosa.'},\n",
       " {'role': 'user', 'content': 'E qual é a sua cor?'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazendo uma nova solicitação para o modelo 'gpt-3.5-turbo-0125' com base nas mensagens acumuladas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - messages: a lista de mensagens (perguntas e respostas) que define o contexto da conversa\n",
    "# - model: o modelo de linguagem a ser usado (neste caso, 'gpt-3.5-turbo-0125')\n",
    "# - max_tokens: o número máximo de tokens que a resposta pode ter\n",
    "# - temperature: define a aleatoriedade da resposta (0 significa respostas mais previsíveis e determinísticas)\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "    messages=mensagens,\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    max_tokens=1000,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exibindo o conteúdo da resposta gerada pelo assistente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vermelha, verde ou amarela.\n"
     ]
    }
   ],
   "source": [
    "# A resposta está localizada na primeira escolha ('choices[0]'), dentro da chave 'message.content'\n",
    "print(resposta.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerando uma função para facilitar o processo\n",
    "\n",
    "A seguir, vamos definir uma função chamada `geracao_texto` que facilita a interação com a API do ChatGPT.\n",
    "Essa função permite enviar mensagens, receber respostas e manter o histórico da conversa de maneira automática.\n",
    "\n",
    "### Parâmetros da Função:\n",
    "- **mensagens**: Lista contendo o histórico de mensagens anteriores.\n",
    "- **model**: O modelo de linguagem a ser utilizado (padrão: `gpt-3.5-turbo-0125`).\n",
    "- **max_tokens**: Define o número máximo de tokens permitidos na resposta (padrão: 1000).\n",
    "- **temperature**: Controla o nível de aleatoriedade da resposta. Um valor de 0 produz respostas mais determinísticas.\n",
    "\n",
    "A função retorna o histórico atualizado das mensagens com a nova resposta incluída.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geracao_texto(mensagens, model='gpt-3.5-turbo-0125', max_tokens=1000, temperature=0):\n",
    "    \"\"\"\n",
    "    Função para gerar texto usando a API do ChatGPT com base nas mensagens passadas.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - mensagens: lista de dicionários que contém as mensagens anteriores (conversa).\n",
    "    - model: o modelo de linguagem a ser usado (padrão: 'gpt-3.5-turbo-0125').\n",
    "    - max_tokens: o número máximo de tokens que a resposta pode ter (padrão: 1000).\n",
    "    - temperature: define a aleatoriedade da resposta (padrão: 0, mais determinístico).\n",
    "    \"\"\"\n",
    "\n",
    "    # Fazendo uma solicitação à API do ChatGPT com base nas mensagens e parâmetros fornecidos.\n",
    "    resposta = client.chat.completions.create(\n",
    "        messages=mensagens,\n",
    "        model=model,  # Modelo de linguagem usado (exemplo: 'gpt-3.5-turbo-0125')\n",
    "        max_tokens=max_tokens,  # Limita o tamanho da resposta\n",
    "        temperature=temperature  # Controla a criatividade da resposta (0 = mais previsível)\n",
    "    )\n",
    "\n",
    "    # Exibindo a resposta gerada pelo assistente\n",
    "    print(resposta.choices[0].message.content)\n",
    "\n",
    "    # Adiciona a resposta gerada pelo assistente à lista de mensagens.\n",
    "    # A linha comentada (mensagens.append...) exigiria que você manualmente dissesse se o conteúdo vem do user ou assistant.\n",
    "    # A linha ativa abaixo usa 'model_dump()' para automaticamente formatar e estruturar a resposta de acordo com o formato correto, excluindo valores None.\n",
    "    mensagens.append(resposta.choices[0].message.model_dump(exclude_none=True))  # Inclui a resposta gerada no histórico de mensagens.\n",
    "\n",
    "    # Retorna a lista atualizada de mensagens com a nova resposta do assistente incluída.\n",
    "    return mensagens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializando a lista de mensagens com uma pergunta do usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A primeira mensagem contém o papel (role) de 'user' e a pergunta \"O que é uma maçã em 5 palavras?\"\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em 5 palavras?'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chamando a função 'geracao_texto' para enviar a mensagem à API e obter a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruta redonda e saborosa.\n"
     ]
    }
   ],
   "source": [
    "# A função retorna o histórico atualizado de mensagens com a resposta do assistente incluída.\n",
    "mensagens = geracao_texto(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando uma nova mensagem do usuário à lista de mensagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O usuário faz uma nova pergunta: \"E qual é a sua cor?\"\n",
    "mensagens.append({'role': 'user', 'content': 'E qual é a sua cor?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chamando novamente a função 'geracao_texto' com a lista atualizada de mensagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vermelha, verde ou amarela.\n"
     ]
    }
   ],
   "source": [
    "# A função envia a nova pergunta para a API e retorna o histórico de mensagens atualizado com a resposta do assistente.\n",
    "mensagens = geracao_texto(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando as diferenças de acordo com a quantidade de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uma maçã é uma fruta redonda, geralmente de cor vermelha,\n",
      "Uma maçã é uma fruta redonda, geralmente de cor vermelha, verde ou amarela, com polpa bran\n",
      "Uma maçã é uma fruta redonda, geralmente de cor vermelha, verde ou amarela, com polpa branca e suculenta. É uma das frutas mais populares e consumidas em todo o mundo, sendo conhecida por seu sabor doce e refrescante. A maçã é rica em fibras, vitaminas e minerais, sendo considerada uma fruta muito saudável. Ela pode ser consum\n"
     ]
    }
   ],
   "source": [
    "# Definindo 'max_tokens=20', limitando a resposta da API a apenas 5 tokens.\n",
    "# Isso ainda resulta em uma resposta curta, mas mais completa que com 2 tokens.\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=20)\n",
    "\n",
    "# Repetindo a pergunta \"O que é uma maçã?\" \n",
    "# Definindo 'max_tokens=30', permitindo uma resposta significativamente maior e mais detalhada.\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=30)\n",
    "\n",
    "# Fazendo uma nova pergunta: \"O que é uma maçã?\"\n",
    "# Definindo 'max_tokens=100' para permitir uma resposta mais longa.\n",
    "# Também ajustando 'temperature=0', garantindo que a resposta seja mais determinística (menos aleatória e mais precisa).\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=100, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando as diferenças de acordo com a 'temperature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruta redonda, suculenta e crocante, geralmente de cor vermelha.\n",
      "Fruta redonda, suculenta e doce, de casca verde ou vermelha.\n",
      "Uma fruta redonda com polpa suculenta e sabor doce.\n"
     ]
    }
   ],
   "source": [
    "# Fazendo uma nova pergunta: \"O que é uma maçã?\"\n",
    "# Definindo 'max_tokens=30' para permitir uma resposta mais longa (apesar da pergunta solicitar no máximo 10 palavras).\n",
    "# Também ajustando 'temperature=0', garantindo uma resposta mais determinística e precisa (menos criativa e mais direta).\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em no maximo 10 palavras?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=30, temperature=0)\n",
    "\n",
    "# Repetindo a pergunta \"O que é uma maçã?\" ,\n",
    "# mas agora com 'temperature=1', o que aumenta a criatividade do modelo,\n",
    "# permitindo respostas mais variadas e menos previsíveis.\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em no maximo 10 palavras?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=30, temperature=1)\n",
    "\n",
    "# Repetindo a pergunta \"O que é uma maçã?\" \n",
    "# Definindo 'temperature=2', o que resulta em uma resposta altamente criativa e potencialmente imprevisível,\n",
    "# aumentando a variedade e a originalidade na geração do texto.\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em no maximo 10 palavras em portugues?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=30, temperature=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando stream na função de geração de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que o parâmetro `stream` faz?\n",
    "\n",
    "O parâmetro `stream`, quando definido como `True`, permite que as respostas da API do ChatGPT sejam transmitidas em tempo real, pedaço por pedaço (chunks), à medida que são geradas. Isso é especialmente útil em casos onde a resposta é muito longa ou quando se deseja atualizar uma interface com novas partes do texto assim que elas ficam disponíveis.\n",
    "\n",
    "#### Benefícios do `stream`:\n",
    "- **Respostas longas**: Para evitar esperar pela resposta completa, o `stream` permite que partes do conteúdo sejam recebidas e exibidas imediatamente.\n",
    "- **Melhor experiência do usuário**: Em uma interface interativa, a exibição\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geracao_texto_e_print_simultaneo(mensagens, model='gpt-3.5-turbo-0125', max_tokens=1000, temperature=0, stream=True):\n",
    "    \"\"\"\n",
    "    Função para gerar texto usando a API do ChatGPT com base nas mensagens passadas.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - mensagens: lista de dicionários que contém as mensagens anteriores (conversa).\n",
    "    - model: o modelo de linguagem a ser usado (padrão: 'gpt-3.5-turbo-0125').\n",
    "    - max_tokens: o número máximo de tokens que a resposta pode ter (padrão: 1000).\n",
    "    - temperature: define a aleatoriedade da resposta (padrão: 0, mais determinístico).\n",
    "    - stream: se True, a resposta é transmitida em tempo real, recebendo pedaços do conteúdo conforme ele é gerado.\n",
    "    \"\"\"\n",
    "\n",
    "    resposta_completa = \"\"\n",
    "\n",
    "    # Fazendo uma solicitação à API do ChatGPT com base nas mensagens e parâmetros fornecidos.\n",
    "    resposta = client.chat.completions.create(\n",
    "        messages=mensagens,\n",
    "        model=model,  # Modelo de linguagem usado (exemplo: 'gpt-3.5-turbo-0125')\n",
    "        max_tokens=max_tokens,  # Limita o tamanho da resposta\n",
    "        temperature=temperature,  # Controla a criatividade da resposta (0 = mais previsível)\n",
    "        stream=stream\n",
    "    )\n",
    "\n",
    "    for stream_resposta in resposta:\n",
    "        texto = stream_resposta.choices[0].delta.content\n",
    "\n",
    "        if texto:\n",
    "            resposta_completa += texto\n",
    "            time.sleep(0.1)\n",
    "            print(texto, end=\"\")\n",
    "\n",
    "    return mensagens, resposta_completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chamando a função 'geracao_texto_e_print_simultaneo' para perguntar e ter a resposta simultaneamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uma maçã é uma fruta redonda, geralmente de cor vermelha, verde ou amarela, com polpa bran"
     ]
    }
   ],
   "source": [
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça?'}]\n",
    "\n",
    "mensagens, resposta_completa = geracao_texto_e_print_simultaneo(mensagens, max_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Uma maçã é uma fruta redonda, geralmente de cor vermelha, verde ou amarela, com polpa bran'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta_completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
