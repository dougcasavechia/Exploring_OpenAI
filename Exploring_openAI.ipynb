{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv  # Para carregar variáveis de ambiente do arquivo .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando as variáveis de ambiente a partir do arquivo .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando as variáveis de ambiente a partir do arquivo .env\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Recuperando a chave da API OpenAI armazenada no arquivo .env\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurando a chave da API para autenticar solicitações à OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo o cliente para interagir com a API OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando uma mensagem de exemplo para a API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui, simulamos uma pergunta do usuário sobre uma maçã\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em 5 palavras?'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazendo uma solicitação para o modelo 'gpt-3.5-turbo-0125'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo uma solicitação para o modelo 'gpt-3.5-turbo-0125'\n",
    "# max_tokens: o máximo de tokens que a resposta pode ter\n",
    "# temperature: controla a criatividade (0 = mais determinístico, menos criativo)\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "    messages=mensagens,\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    max_tokens=1000,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exibindo a resposta gerada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruta redonda e saborosa.\n"
     ]
    }
   ],
   "source": [
    "print(resposta.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando a resposta do assistente à lista de mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função append insere a resposta do modelo como se fosse o \"assistant\" respondendo\n",
    "mensagens.append({'role': 'assistant', 'content': resposta.choices[0].message.content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando uma nova mensagem do usuário à lista de mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa mensagem simula o usuário perguntando \"E qual é a sua cor?\"\n",
    "# A mensagem é adicionada com o papel ('role') de 'user', mantendo o fluxo da conversa.\n",
    "mensagens.append({'role': 'user', 'content': 'E qual é a sua cor?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exibindo a respostas geradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'O que é uma maça em 5 palavras?'},\n",
       " {'role': 'assistant', 'content': 'Fruta redonda e saborosa.'},\n",
       " {'role': 'user', 'content': 'E qual é a sua cor?'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazendo uma nova solicitação para o modelo 'gpt-3.5-turbo-0125' com base nas mensagens acumuladas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vermelha, verde ou amarela.\n"
     ]
    }
   ],
   "source": [
    "# - messages: a lista de mensagens (perguntas e respostas) que define o contexto da conversa\n",
    "# - model: o modelo de linguagem a ser usado (neste caso, 'gpt-3.5-turbo-0125')\n",
    "# - max_tokens: o número máximo de tokens que a resposta pode ter\n",
    "# - temperature: define a aleatoriedade da resposta (0 significa respostas mais previsíveis e determinísticas)\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "    messages=mensagens,\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    max_tokens=1000,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exibindo o conteúdo da resposta gerada pelo assistente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruta redonda e saborosa.\n"
     ]
    }
   ],
   "source": [
    "# A resposta está localizada na primeira escolha ('choices[0]'), dentro da chave 'message.content'\n",
    "print(resposta.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerando uma função para facilitar o processo\n",
    "\n",
    "A seguir, vamos definir uma função chamada `geracao_texto` que facilita a interação com a API do ChatGPT.\n",
    "Essa função permite enviar mensagens, receber respostas e manter o histórico da conversa de maneira automática.\n",
    "\n",
    "### Parâmetros da Função:\n",
    "- **mensagens**: Lista contendo o histórico de mensagens anteriores.\n",
    "- **model**: O modelo de linguagem a ser utilizado (padrão: `gpt-3.5-turbo-0125`).\n",
    "- **max_tokens**: Define o número máximo de tokens permitidos na resposta (padrão: 1000).\n",
    "- **temperature**: Controla o nível de aleatoriedade da resposta. Um valor de 0 produz respostas mais determinísticas.\n",
    "\n",
    "A função retorna o histórico atualizado das mensagens com a nova resposta incluída.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geracao_texto(mensagens, model='gpt-3.5-turbo-0125', max_tokens=1000, temperature=0):\n",
    "    \"\"\"\n",
    "    Função para gerar texto usando a API do ChatGPT com base nas mensagens passadas.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - mensagens: lista de dicionários que contém as mensagens anteriores (conversa).\n",
    "    - model: o modelo de linguagem a ser usado (padrão: 'gpt-3.5-turbo-0125').\n",
    "    - max_tokens: o número máximo de tokens que a resposta pode ter (padrão: 1000).\n",
    "    - temperature: define a aleatoriedade da resposta (padrão: 0, mais determinístico).\n",
    "    \"\"\"\n",
    "\n",
    "    # Fazendo uma solicitação à API do ChatGPT com base nas mensagens e parâmetros fornecidos.\n",
    "    resposta = client.chat.completions.create(\n",
    "        messages=mensagens,\n",
    "        model=model,  # Modelo de linguagem usado (exemplo: 'gpt-3.5-turbo-0125')\n",
    "        max_tokens=max_tokens,  # Limita o tamanho da resposta\n",
    "        temperature=temperature  # Controla a criatividade da resposta (0 = mais previsível)\n",
    "    )\n",
    "\n",
    "    # Exibindo a resposta gerada pelo assistente\n",
    "    print(resposta.choices[0].message.content)\n",
    "\n",
    "    # Adiciona a resposta gerada pelo assistente à lista de mensagens.\n",
    "    # A linha comentada (mensagens.append...) exigiria que você manualmente dissesse se o conteúdo vem do user ou assistant.\n",
    "    # A linha ativa abaixo usa 'model_dump()' para automaticamente formatar e estruturar a resposta de acordo com o formato correto, excluindo valores None.\n",
    "    mensagens.append(resposta.choices[0].message.model_dump(exclude_none=True))  # Inclui a resposta gerada no histórico de mensagens.\n",
    "\n",
    "    # Retorna a lista atualizada de mensagens com a nova resposta do assistente incluída.\n",
    "    return mensagens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializando a lista de mensagens com uma pergunta do usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A primeira mensagem contém o papel (role) de 'user' e a pergunta \"O que é uma maçã em 5 palavras?\"\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em 5 palavras?'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chamando a função 'geracao_texto' para enviar a mensagem à API e obter a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruta redonda e saborosa.\n"
     ]
    }
   ],
   "source": [
    "# A função retorna o histórico atualizado de mensagens com a resposta do assistente incluída.\n",
    "mensagens = geracao_texto(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando uma nova mensagem do usuário à lista de mensagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O usuário faz uma nova pergunta: \"E qual é a sua cor?\"\n",
    "mensagens.append({'role': 'user', 'content': 'E qual é a sua cor?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chamando novamente a função 'geracao_texto' com a lista atualizada de mensagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vermelha, verde ou amarela.\n"
     ]
    }
   ],
   "source": [
    "# A função envia a nova pergunta para a API e retorna o histórico de mensagens atualizado com a resposta do assistente.\n",
    "mensagens = geracao_texto(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando max_tokens e temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruta\n"
     ]
    }
   ],
   "source": [
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em 5 palavras?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruta redonda\n"
     ]
    }
   ],
   "source": [
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em 5 palavras?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando as diferenças de acordo com a quantidade de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruta redonda,\n",
      "Fruta redonda, suculenta, doce, crocante\n",
      "Fruta redonda, suculenta e crocante, geralmente de cor vermelha.\n"
     ]
    }
   ],
   "source": [
    "# Definindo 'max_tokens=5', limitando a resposta da API a apenas 5 tokens.\n",
    "# Isso ainda resulta em uma resposta curta, mas mais completa que com 2 tokens.\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em 10 palavras?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=5)\n",
    "\n",
    "# Repetindo a pergunta \"O que é uma maçã em 10 palavras?\" \n",
    "# Definindo 'max_tokens=15', permitindo uma resposta significativamente maior e mais detalhada.\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em 10 palavras?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=15)\n",
    "\n",
    "# Fazendo uma nova pergunta: \"O que é uma maçã em no máximo 10 palavras?\"\n",
    "# Definindo 'max_tokens=20' para permitir uma resposta mais longa.\n",
    "# Também ajustando 'temperature=0', garantindo que a resposta seja mais determinística (menos aleatória e mais precisa).\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em no maximo 10 palavras?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=30, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando as diferenças de acordo com a 'temperature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruta redonda, suculenta e crocante, geralmente de cor vermelha.\n",
      "Fruta redonda, suculenta, com casca vermelha, amarela e verde.\n",
      "Uma fruta madura, carnuda macc totalmente portimoncommonupertgomolia排 rayaben]').tagName.               Splche ladymobxmoire\n"
     ]
    }
   ],
   "source": [
    "# Fazendo uma nova pergunta: \"O que é uma maçã em no máximo 10 palavras?\"\n",
    "# Definindo 'max_tokens=30' para permitir uma resposta mais longa (apesar da pergunta solicitar no máximo 10 palavras).\n",
    "# Também ajustando 'temperature=0', garantindo uma resposta mais determinística e precisa (menos criativa e mais direta).\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em no maximo 10 palavras?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=30, temperature=0)\n",
    "\n",
    "# Fazendo a mesma pergunta \"O que é uma maçã em no máximo 10 palavras?\",\n",
    "# mas agora com 'temperature=1', o que aumenta a criatividade do modelo,\n",
    "# permitindo respostas mais variadas e menos previsíveis.\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em no maximo 10 palavras?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=30, temperature=1)\n",
    "\n",
    "# Fazendo uma pergunta ligeiramente modificada: \"O que é uma maçã em no máximo 10 palavras em português?\"\n",
    "# Definindo 'temperature=2', o que resulta em uma resposta altamente criativa e potencialmente imprevisível,\n",
    "# aumentando a variedade e a originalidade na geração do texto.\n",
    "mensagens = [{'role' : 'user', 'content' : 'O que é uma maça em no maximo 10 palavras em portugues?'}]\n",
    "mensagens = geracao_texto(mensagens, max_tokens=30, temperature=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
